---
title: "Model Assessment and Selection"
author: "Marjorie Blanco"
date: "11/2/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(moderndive)
library(ggplot2)
library(dplyr)
```

# Model Assessment and Selection

```{r}
# log10() transform price and size
house_prices <- house_prices %>%
  mutate(
    log10_price = log10(price),
    log10_size = log10(sqft_living)
  )

model_price_1 <- lm(log10_price ~ log10_size + yr_built,
                    data = house_prices)

model_price_3 <- lm(log10_price ~ log10_size + condition,
                    data = house_prices)

# Model 2
model_price_2 <- lm(log10_price ~ log10_size + bedrooms, 
                    data = house_prices)

# Calculate squared residuals
get_regression_points(model_price_2) %>%
mutate(sq_residuals = residual^2) %>%
  summarize(sum_sq_residuals = sum(sq_residuals))

# Model 4
model_price_4 <- lm(log10_price ~ log10_size + waterfront, 
                    data = house_prices)

# Calculate squared residuals
get_regression_points(model_price_4) %>%
  mutate(sq_residuals = residual^2) %>%
  summarize(sum_sq_residuals = sum(sq_residuals))

# Which model to select?
# Based on these two values of the sum of squared residuals, which of these two models do you think is "better", and hence which would you select?
# 
# model_price_2 that uses log10_size and bedrooms?
# model_price_4 that uses log10_size and waterfront?
# Since model_price_4's value was 599, select this one.

# Fit model
model_price_2 <- lm(log10_price ~ log10_size + bedrooms,
                    data = house_prices)
                    
# Get fitted/values & residuals, compute R^2 using residuals
get_regression_points(model_price_2) %>%
  summarize(r_squared = 1 - var(residual)/ var(log10_price))

#You observed an R-squared value of 0.465, which means that 46.5% of the total variability of the outcome variable log base 10 price can be explained by this model.

# Fit model
model_price_4 <- lm(log10_price ~ log10_size + waterfront,
                    data = house_prices)

# Get fitted/values & residuals, compute R^2 using residuals
get_regression_points(model_price_4) %>% 
  summarize(r_squared = 1 - var(residual)/ var(log10_price))


#Since model_price_4 had a higher R2 of 0.470, it "fit" the data better.
```


```{r}
# Model 2
model_price_1 <- lm(log10_price ~ log10_size + yr_built, 
                    data = house_prices)

# Calculate squared residuals
get_regression_points(model_price_1) %>%
  summarize(r_squared = 1 - var(residual)/ var(log10_price))

# Model 3
model_price_1 <- lm(log10_price ~ log10_size + condition, 
                    data = house_prices)

# Calculate squared residuals
get_regression_points(model_price_3) %>%
  summarize(r_squared = 1 - var(residual)/ var(log10_price))

new_houses <- data.frame(
  log10_size = c(2.9, 3.1),
  condition = factor(c(3, 4))
)

get_regression_points(model_price_3, newdata = new_houses)


get_regression_points(model_price_3) %>%
  mutate(sq_residuals = residual^2) %>%
  summarize(mse = mean(sq_residuals)) %>%
  mutate(rmse = sqrt(mse))


# Get all residuals, square them, take the mean and square root
get_regression_points(model_price_2) %>%
  mutate(sq_residuals = residual^2) %>%
  summarize(mse = mean(sq_residuals)) %>% 
  mutate(rmse = sqrt(mse))

#the RMSE is 0.167. You can think of this as the “typical” prediction error this model makes.


get_regression_points(model_price_2) %>%
  mutate(sq_residuals = residual^2) %>%
  summarize(mse = mean(sq_residuals), rmse = sqrt(mean(sq_residuals)))

#the RMSE is 0.166. You can think of this as the “typical” prediction error this model makes.
#Since model_price_4 had a lower rmse of 0.166, this is suggestive that this model has better preditive power.

# Randomly shuffle order of rows:
house_prices_shuffled <- house_prices %>%
 sample_frac(size = 1, replace = FALSE)
# Split into train and test:

train <- house_prices_shuffled %>%
slice(1:10000)

test <- house_prices_shuffled %>%
slice(10001:21613)

train_model_price_1 <- lm(log10_price ~ log10_size + yr_built,
data = train)
get_regression_table(train_model_price_1)
get_regression_points(train_model_price_1, newdata = test)


# Get predictions and compute RMSE:
get_regression_points(train_model_price_1, newdata = test) %>%
mutate(sq_residuals = residual^2) %>%
summarize(rmse = sqrt(mean(sq_residuals)))

# Train model:
train_model_price_3 <- lm(log10_price ~ log10_size + condition,
data = train)
# Get predictions and compute RMSE:
get_regression_points(train_model_price_3, newdata = test) %>%
mutate(sq_residuals = residual^2) %>%
summarize(rmse = sqrt(mean(sq_residuals)))


# Set random number generator seed value for reproducibility
set.seed(76)

# Randomly reorder the rows
house_prices_shuffled <- house_prices %>% 
  sample_frac(size = 1, replace = FALSE)

# Train/test split
train <- house_prices_shuffled %>%
  slice(1:10000)
test <- house_prices_shuffled %>%
  slice(10001:21613)


# Fit model to training set
train_model_2 <- lm(log10_price~log10_size + bedrooms, data = train)


# Make predictions on test set
get_regression_points(train_model_2, newdata = test)

# Compute RMSE
get_regression_points(train_model_2, newdata = test) %>% 
  mutate(sq_residuals = residual^2) %>%
  summarize(rmse = sqrt(mean(sq_residuals)))

#your RMSE using size and condition as predictor variables is 0.167, which is higher than 0.165 when you used size and year built! It seems the latter is marginally better!


```

